# A list of papers about audio captioning

## Introduction 

Audio captioning is a novel and exciting research direction, 
focusing on the automatic generation of textual descriptions
(i.e. captions) for general audio. This repository is a list
of papers that are focusing on audio captioning. 

The papers are groupped according to the year that are published,
and for each paper there are: 

* The full citation
* Links to the PDF, e.g. arXiv and/or publisher web site (if applicable)
* A link to the GitHub repository (if applicable)
* A link to pre-trained model(s) (if applicable)

The ordering is descending, having the newer papers first. 

If you know of an audio captioning paper that is not included
in this list, please make an issue and it will be included!

Enjoy! 

P.S. This repository is maintained by
[K. Drossos](https://github.com/dr-costas). 

----

## Table of contents

1. [Year 2020](#year-2020)
1. [Year 2019](#year-2019)
1. [Year 2017](#year-2017)
  
----

## Year 2020

### Clotho

---

---

## Year 2019

### Crowdsourcing a Dataset of Audio Captions

<dl>
  <dt>Citation</dt>
  <dd>S. Lipping, K. Drossos, and T. Virtanen, "Crowdsourcing a
  dataset of audio captions," inDetection and Classification of
  Acoustic Scenes and Events (DCASE) 2019, Oct. 2019
  </dd>

  <dt>Paper links</dt>
  <dd>
  <a href="https://arxiv.org/abs/1907.09238">arXiv</a>

  <a href="http://dcase.community/documents/workshop2019/proceedings/DCASE2019Workshop_Lipping_31.pdf">DCASE</a>
  </dd>

  <dt>
  <details><summary>BibTex entry</summary><br>

	@INPROCEEDINGS{lipping:2019:dcase,
    author={S. Lipping and K. Drossos and T. Virtanen},
    title={Crowdsourcing a Dataset of Audio Captions},
	booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019)},
    address = {New York University, NY, USA},
    month = {Oct.},
    year = {2019},
    pages = {139--143},
    ISSN={2379-190X}}

  </details>
  </dt>
</dl>

----

### Neural Audio Captioning Based On Conditional Sequence-to-Sequence Model

<dl>
  <dt>Citation</dt>
  <dd>Shota Ikawa and Kunio Kashino, "Neural Audio Captioning Based
  On Conditional Sequence-to-Sequence Model," in Workshop of Detection
  and Classification of Acoustic Scenes and Events (DCASE), Oct.
  2019.
  </dd>

  <dt>Paper links</dt>
  <dd>
  <a href="http://dcase.community/documents/workshop2019/proceedings/DCASE2019Workshop_Ikawa_82.pdf">DCASE</a>
  </dd>

  <dt>
  <details><summary>BibTex entry</summary><br>

	@inproceedings{ikawa:2019:dcase,
    author = {Shota Ikawa and Kunio Kashino},
    title = {Neural Audio Captioning Based On Conditional Sequence-to-Sequence Model},
    booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events 2019 Workshop ({DCASE2019})},
    address = {New York University, NY, USA},
    month = {Oct.},
    year = {2019},
    pages = {99--103},
	ISSN={2379-190X}}

  </details>
  </dt>
</dl>

----

### AudioCaps: Generating captions for audios in the wild

<dl>
  <dt>Citation</dt>
  <dd>C. D. Kim, B. Kim, H. Lee, and G. Kim, "AudioCaps:
  Generating captions for audios in the wild,” in Proceedings
  of the 2019 Conference of the North American Chapter of the
  Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers), Minneapolis,
  Minnesota, Jun. 2019, pp. 119–132, Association for Computational
  Linguistics 
  </dd>

  <dt>Paper links</dt>
  <dd>
  <a href="https://arxiv.org/abs/1706.10006">acwlweb</a>
  </dd>

  <dt>Code</dt>
  <dd>
  <a href="https://github.com/cdjkim/audiocaps">GitHub</a>
  </dd>

  <dt>Data</dt>
  <dd>
  <a href="https://github.com/cdjkim/audiocaps/blob/master/dataset/README.md">GitHub</a>
  </dd>

  <dt>
  <details><summary>BibTex entry</summary><br>

	@inproceedings{kim:2019:nacacl,
    title = {{A}udio{C}aps: Generating Captions for Audios in The Wild},
    author = {C. D. Kim and B. Kim and H. Lee and G. Ki}",
    booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
    month = {Jun.},
    year = {2019},
    address = {Minneapolis, Minnesota},
    publisher = {Association for Computational Linguistics},
    doi = {10.18653/v1/N19-1011},
    pages = {119--132}}

  </details>
  </dt>
</dl>

----

### Audio caption: Listen and tell

<dl>
  <dt>Citation</dt>
  <dd>M. Wu, H. Dinkel, and K. Yu, "Audio caption: Listen and
  tell," in 2019 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP), May 2019, pp. 830–834
  </dd>

  <dt>Paper links</dt>
  <dd>
  <a href="https://arxiv.org/abs/1706.10006">arXiv</a>

  <a href="https://ieeexplore.ieee.org/document/8170058">ieeexplore</a>
  </dd>

  <dt>
  <details><summary>BibTex entry</summary><br>

	@inproceedings{wu:2019:icassp,
    author={M. {Wu} and H. {Dinkel} and K. {Yu}},
    booktitle={2019 IEEE International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
    title={Audio Caption: Listen and Tell},
    year={2019},
    pages={830-834},
    doi={10.1109/ICASSP.2019.8682377},
    ISSN={2379-190X},
    month={May}}

  </details>
  </dt>
</dl>

## Year 2017

----

----

### Automated Audio Captioning with Recurrent Neural Networks


<dl>
  <dt>Citation</dt>
  <dd>K. Drossos, S. Adavanne, and T. Virtanen, "Automated audio
  captioning with recurrent neural networks," in 2017 IEEE Workshop
  on Applications of Signal Processing to Audio and Acoustics
  (WASPAA), Oct. 2017, pp. 374–378</dd>

  <dt>Paper links</dt>
  <dd>
  <a href="https://arxiv.org/abs/1706.10006">arXiv</a>

  <a href="https://ieeexplore.ieee.org/document/8170058">ieeexplore</a>
  </dd>

  <dt>
  <details><summary>BibTex entry</summary><br>

    @inproceedings{drossos:2017:waspaa,
    author={K. {Drossos} and S. {Adavanne} and T. {Virtanen}},
    booktitle={2017 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},
    title={Automated audio captioning with recurrent neural networks},
    year={2017},
    pages={374-378}}

  </details>
  </dt>
</dl>

