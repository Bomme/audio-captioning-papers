# A list of papers about audio captioning

## Introduction 

Audio captioning is a novel and exciting research direction, 
focusing on the automatic generation of textual descriptions
(i.e. captions) for general audio. This repository is a list
of papers that are focusing on audio captioning. 

The papers are groupped according to the year that are published,
and for each paper there are: 

* The full reference
* Links to the PDF, e.g. arXiv and/or publisher web site (if applicable)
* BibTex entry
* A link to the GitHub repository (if applicable)
* A link to pre-trained model(s) (if applicable)

The ordering is descending, having the newer papers first. 

If you know of an audio captioning paper that is not included
in this list, please make an issue or a pull requrest and it
will be included!

Enjoy! 

P.S. This repository is maintained by
[K. Drossos](https://github.com/dr-costas). 

P.S.2 The order is roughly chronological. If there is a
suggestion for changing the ordering of specific papers,
please feel free to create an issue.

----

## Table of contents

1. [Year 2020](#year-2020)
1. [Year 2019](#year-2019)
1. [Year 2017](#year-2017)
  
# Year 2020
#### Effects of Word-frequency based Pre- and Post- Processings for Audio Captioning

<details><summary>Information</summary>
 <dl>
   <dt>Reference</dt>
   <dd>D. Takeuchi, Y. Koizumi, Y. Ohishi, N. Harada, and K. Kashino,
   "Effects of Word-frequency based Pre- and Post- Processings for Audio Captioning,"
   in Detection and Classification of Acoustic Scenes and Events (DCASE) 2020,
   Nov. 2020
   </dd>
 
   <dt>Paper links</dt>
   <dd>
   <a href="https://arxiv.org/abs/2009.11436">arXiv</a>
   </dd>

   <dt>BibTex entry</dt>

   <dd>

	@inproceedings{takeuchi:2020:dcase,
    title={Effects of Word-frequency based Pre- and Post- Processings for Audio Captioning},
    author={D. Takeuchi and Y. Koizumi and Y. Ohishi and N. Harada and and K. Kashino},
    year={2020},
    booktitle={Detection and Classification of Acoustic Scenes and Events ({DCASE})},
    month = {Nov.},}
    
   </dd>
 </dl>

----

</details>

#### A Transformer-based Audio Captioning Model with Keyword Estimation

<details><summary>Information</summary>
 <dl>
   <dt>Reference</dt>
   <dd>Y. Koizumi, R. Masumura, K. Nishida, M. Yasuda, and S. Saito,
   "A Transformer-based Audio Captioning Model with Keyword Estimation,"
   in INTERSPEECH, 2020
   </dd>
 
   <dt>Paper links</dt>
   <dd>
   <a href="https://arxiv.org/abs/2007.00222">arXiv</a>
   </dd>

   <dt>BibTex entry</dt>

   <dd>

	@inproceedings{koizumi:2020:interspeech,
    title={A Transformer-based Audio Captioning Model with Keyword Estimation},
    author={Y. Koizumi and R. Masumura and K. Nishida and M. Yasuda and S. Saito},
    year={2020},
    booktitle={INTERSPEECH 2020},
    month={Oct.}}

   </dd>
 </dl>

----

</details>

#### Multi-task Regularization Based on Infrequent Classes for Audio Captioning

<details><summary>Information</summary>
 <dl>
   <dt>Reference</dt>
   <dd>E. Çakır, K. Drossos, and T. Virtanen, 
   "Multi-task Regularization Based on Infrequent Classes for Audio Captioning,"
   in Detection and Classification of Acoustic Scenes and Events (DCASE) 2020,
   Nov. 2020
   </dd>
 
   <dt>Paper links</dt>
   <dd>
   <a href="https://arxiv.org/abs/2007.04660">arXiv</a>
   </dd>

   <dt>BibTex entry</dt>

   <dd>

	@inproceedings{cakir:2020:arxiv-a,
    title={Multi-task Regularization Based on Infrequent Classes for Audio Captioning},
    author={E. \c{C}ak{\i}r and K. Drossos and T. Virtanen},
    year={2020},
    booktitle={Detection and Classification of Acoustic Scenes and Events ({DCASE})},
    month = {Nov.},}

   </dd>
 </dl>

----

</details>

#### Temporal Sub-sampling of Audio Feature Sequences for Automated Audio Captioning

<details><summary>Information</summary>
 <dl>
   <dt>Reference</dt>
   <dd>K. Nguyen, K. Drossos, and T. Virtanen,
   "Temporal Sub-sampling of Audio Feature Sequences for Automated Audio Captioning,"
   in Detection and Classification of Acoustic Scenes and Events (DCASE) 2020,
   Nov. 2020
   </dd>
 
   <dt>Paper links</dt>
   <dd>
   <a href="https://arxiv.org/abs/2007.02676">arXiv</a>
   </dd>

   <dt>BibTex entry</dt>

   <dd>

	@inproceedings{nguyen:2020:arxiv-a,
    title={Temporal Sub-sampling of Audio Feature Sequences for Automated Audio Captioning},
    author={K. Nguyen and K. Drossos and T. Virtanen},
    year={2020},
    booktitle={Detection and Classification of Acoustic Scenes and Events ({DCASE})},
    month = {Nov.},}

   </dd>
 </dl>

----

</details>

#### The SJTU Submission for DCASE2020 Task 6: A CRNN-GRU Based Reinforcement Learning Approach to Audiocaption

<details><summary>Information</summary>
 <dl>
   <dt>Reference</dt>
   <dd>X. Xu, H. Dinkel, M. Wu, and K. Yu,
   "The SJTU Submission for DCASE2020 Task 6: A CRNN-GRU
   Based Reinforcement Learning Approach to Audiocaption,"
   DCASE2020 Challenge, Tech. Rep., Jun. 2020
   </dd>
 
   <dt>Paper links</dt>
   <dd>
   <a href="http://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Xu_43_t6.pdf">DCASE</a>
   </dd>

   <dt>BibTex entry</dt>

   <dd>

    @techreport{xu:2020:dcase:tech-report,
    author = {X. Xu and H. Dinkel and M. Wu, and K. Yu},
    title = {The SJTU Submission for DCASE2020 Task 6: A CRNN-GRU Based Reinforcement Learning Approach to Audiocaption},
    institution = {DCASE2020 Challenge},
    year = {2020},
    month = {Jun.}}

   </dd>
 </dl>

----

</details>

#### Audio Captioning Based on Transformer and Pre-Training for 2020 DCASE Audio Captioning Challenge

<details><summary>Information</summary>
 <dl>
   <dt>Reference</dt>
   <dd>Y. Wu, K. Chen, Z. Wang, X. Zhang, F. Nian, S. Li, and X. Shao,
   "Audio Captioning Based on Transformer and Pre-Training for
   2020 DCASE Audio Captioning Challenge,"
   DCASE2020 Challenge, Tech. Rep., Jun. 2020
   </dd>
 
   <dt>Paper links</dt>
   <dd>
   <a href="http://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Wu_136_t6.pdf">DCASE</a>
   </dd>

   <dt>Code</dt>
   <dd>
   <a href="https://github.com/lukewys/dcase_2020_T6">GitHub</a>
   </dd>

   <dt>Data</dt>
   <dd>
   <a href="https://github.com/lukewys/dcase_2020_T6">GitHub</a>
   </dd>

   <dt>BibTex entry</dt>

   <dd>

    @techreport{wu-y:2020:dcase:tech-report,
    author = {Y. Wu, K. Chen, Z. Wang, X. Zhang, F. Nian, S. Li, and X. Shao},
    title = {Audio Captioning Based on Transformer and Pre-Training for 2020 DCASE Audio Captioning Challenge},
    institution = {DCASE2020 Challenge},
    year = {2020},
    month = {Jun.}}

   </dd>
 </dl>

----

</details>

#### Automatic Audio Captioning System Based on Convolutional Neural Network

<details><summary>Information</summary>
 <dl>
   <dt>Reference</dt>
   <dd>Q. Wu, S. Tao, and X. Yang,
   "Automatic Audio Captioning System Based on Convolutional Neural Network,"
   DCASE2020 Challenge, Tech. Rep., Jun. 2020
   </dd>
 
   <dt>Paper links</dt>
   <dd>
   <a href="http://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Wu_32_t6.pdf">DCASE</a>
   </dd>

   <dt>Code</dt>
   <dd>
   <a href="https://github.com/SolarQY/dcase-2020-Wu_UESTC_task6_1-master">GitHub</a>
   </dd>

   <dt>Data</dt>
   <dd>
   <a href="https://zenodo.org/record/3876464">Zenodo</a>
   </dd>

   <dt>BibTex entry</dt>

   <dd>

    @techreport{wu-q:2020:dcase:tech-report,
    author = {Q. Wu and S. Tao and X. Yang},
    title = {Automatic Audio Captioning System Based on Convolutional Neural Network},
    institution = {DCASE2020 Challenge},
    year = {2020},
    month = {Jun.}}

   </dd>
 </dl>

----

</details>

#### Automated Audio Captioning With Temporal Attention

<details><summary>Information</summary>
 <dl>
   <dt>Reference</dt>
   <dd>H. Wang, B. Yang, Y. Zou, and D. Chong,
   "Automated Audio Captioning With Temporal Attention,"
   DCASE2020 Challenge, Tech. Rep., Jun. 2020
   </dd>
 
   <dt>Paper links</dt>
   <dd>
   <a href="http://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Wang_5_t6.pdf">DCASE</a>
   </dd>

   <dt>BibTex entry</dt>

   <dd>

    @techreport{wang:2020:dcase:tech-report,
    author = {H. Wang and B. Yang and  Y. Zou and D. Chong},
    title = {Automated Audio Captioning With Temporal Attention},
    institution = {DCASE2020 Challenge},
    year = {2020},
    month = {Jun.}}

   </dd>
 </dl>

----

</details>

#### Audio Captioning With the Transformer

<details><summary>Information</summary>
 <dl>
   <dt>Reference</dt>
   <dd>Anna Shi, "Audio Captioning With the Transformer,"
   DCASE2020 Challenge, Tech. Rep., Jun. 2020
   </dd>
 
   <dt>Paper links</dt>
   <dd>
   <a href="http://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Shi_8_t6.pdf">DCASE</a>
   </dd>

   <dt>BibTex entry</dt>

   <dd>

    @techreport{shi:2020:dcase:tech-report,
    author = {A. Shi},
    title = {Audio Captioning With the TransformerAutomated Audio Captioning},
    institution = {DCASE2020 Challenge},
    year = {2020},
    month = {Jun.}}

   </dd>
 </dl>

----

</details>

#### Automated Audio Captioning

<details><summary>Information</summary>
 <dl>
   <dt>Reference</dt>
   <dd>A. Sampathkumar and D. Kowerko,
   "Automated Audio Captioning," DCASE2020 Challenge,
   Tech. Rep., Jun. 2020
   </dd>
 
   <dt>Paper links</dt>
   <dd>
   <a href="http://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Sampathkumar_44_t6.pdf">DCASE</a>
   </dd>

   <dt>BibTex entry</dt>

   <dd>

    @techreport{sampathkumar:2020:dcase:tech-report,
    author = {A. Sampathkumar and D. Kowerko},
    title = {Automated Audio Captioning},
    institution = {DCASE2020 Challenge},
    year = {2020},
    month = {Jun.}}

   </dd>
 </dl>

----

</details>

#### IRIT-UPS DCASE 2020 audio captioning system

<details><summary>Information</summary>
 <dl>
   <dt>Reference</dt>
   <dd>Thomas Pellegrini, "IRIT-UPS DCASE 2020
   audio captioning system," DCASE2020 Challenge,
   Tech. Rep., Jun. 2020
   </dd>
 
   <dt>Paper links</dt>
   <dd>
   <a href="http://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Pellegrini_131_t6.pdf">DCASE</a>
   </dd>

   <dt>Code</dt>
   <dd>
   <a href="https://github.com/topel/listen-attend-tell">GitHub</a>
   </dd>

   <dt>Data</dt>
   <dd>
   <a href="https://zenodo.org/record/3893974#.Xwr_9S2w3OQ">Zenodo</a>
   </dd>

   <dt>BibTex entry</dt>

   <dd>

    @techreport{pellegrini:2020:dcase:tech-report,
    author = {Thomas Pellegrini},
    title = {IRIT-UPS DCASE 2020 audio captioning system},
    institution = {DCASE2020 Challenge},
    year = {2020},
    month = {Jun.}}

   </dd>
 </dl>

----

</details>

#### Task 6 DCASE 2020: Listen Carefully and Tell: An Audio Captioning System Based on Residual Learning and Gammatone Audio Representation

<details><summary>Information</summary>
 <dl>
   <dt>Reference</dt>
   <dd>J. Naranjo-Alcaza, S. Perez-Castanos,
   P. Zuccarello, and M. Cobos, "Task 6 DCASE 2020:
   Listen Carefully and Tell: An Audio Captioning
   System Based on Residual Learning and Gammatone
   Audio Representation," DCASE2020 Challenge,
   Tech. Rep., Jun. 2020
   </dd>
 
   <dt>Paper links</dt>
   <dd>
   <a href="http://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Naranjo_Alcazar_34_t6.pdf">DCASE</a>
   </dd>

   <dt>BibTex entry</dt>

   <dd>

    @techreport{naranjo-alcazar:2020:dcase:tech-report,
    author = {J. Naranjo-Alcazar and S. Perez-Castanos and P. Zuccarello and M. Cobos},
    title = {Task 6 DCASE 2020: Listen Carefully and Tell: An Audio Captioning System Based on Residual Learning and Gammatone Audio Representation},
    institution = {DCASE2020 Challenge},
    year = {2020},
    month = {Jun.}}

   </dd>
 </dl>

----

</details>

#### Automated Audio Captioning

<details><summary>Information</summary>
 <dl>
   <dt>Reference</dt>
   <dd>N. Kuzmin and A. Dyakonov, "Automated Audio Captioning,"
   DCASE2020 Challenge, Tech. Rep., Jun. 2020
   </dd>
 
   <dt>Paper links</dt>
   <dd>
   <a href="http://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Kuzmin_137_t6.pdf">DCASE</a>
   </dd>

   <dt>Code</dt>
   <dd>
   <a href="https://github.com/paniquex/Automated_Audio_Captioning_DCASE2020">GitHub</a>
   </dd>

   <dt>Data</dt>
   <dd>
   <a href="https://zenodo.org/record/3895543#.Xwr6Ny2w3OQ">Zenodo</a>
   </dd>
 
   <dt>BibTex entry</dt>

   <dd>

    @techreport{kuzmin:2020:dcase:tech-report,
    author = {N. Kuzmin and A. Dyakonov},
    title = {Automated Audio Captioning},
    institution = {DCASE2020 Challenge},
    year = {2020},
    month = {Jun.}}

   </dd>
 </dl>

----

</details>

#### The NTT DCASE2020 Challenge Task 6 System: Automated Audio Captioning With Keywords and Sentence Length Estimation

<details><summary>Information</summary>
 <dl>
   <dt>Reference</dt>
   <dd>Y. Koizumi, D. Takeuchi, Y. Ohishi, N. Harada, and K. Kashino,
   "The NTT DCASE2020 Challenge Task 6 System: Automated Audio
   Captioning With Keywords and Sentence Length Estimation,"
   DCASE2020 Challenge, Tech. Rep., Jun. 2020
   </dd>
 
   <dt>Paper links</dt>
   <dd>
   <a href="https://arxiv.org/abs/2007.00225">arXiv</a>

   <a href="http://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Koizumi_63_t6.pdf">DCASE</a>
   </dd>
 
   <dt>BibTex entry</dt>

   <dd>
 
	 @techreport{koizumi:2020:dcase:tech-report,
     author = {Y. Koizumi and D. Takeuchi and Y. Ohishi and N. Harada and K. Kashino},
     title = {The {NTT} {DCASE2020} Challenge Task 6 System: Automated Audio Captioning With Keywords and Sentence Length Estimation},
     institution = {DCASE2020 Challenge},
     year = {2020},
     month = {Jun.}}
 
   </dd>
 </dl>

----

</details>

#### Audio Captioning using Gated Recurrent Units

<details><summary>Information</summary>
 <dl>
   <dt>Reference</dt>
   <dd>A. O. Eren and M. Sert, "Audio Captioning using Gated Recurrent
   Units," in arXiv:2006.03391 [cs.SD], 2020
   </dd>
 
   <dt>Paper links</dt>
   <dd>
   <a href="https://arxiv.org/abs/2006.03391">arXiv</a>
   </dd>
 
   <dt>BibTex entry</dt>

   <dd>
 
    @misc{eren:2020:arxiv,
    title={Audio Captioning using Gated Recurrent Units},
    author={Ay\c{s}eg\"{u}}l \"{O}zkaya Eren and Mustafa Sert},
    year={2020},
    eprint={2006.03391},
    archivePrefix={arXiv},
    primaryClass={cs.SD}}
 
   </dd>
 </dl>

----

</details>

#### Clotho: An Audio Captioning Dataset

<details><summary>Information</summary>
 <dl>
   <dt>Reference</dt>
   <dd>K. Drossos, S. Lipping, and T. Virtanen, "Clotho:
   An audio captioning dataset," in ICASSP 2020-2020 IEEE
   International Conference on Acoustics, Speech and Signal
   Processing (ICASSP). IEEE, 2020, pp. 736–740
   </dd>
 
   <dt>Paper links</dt>
   <dd>
   <a href="https://arxiv.org/abs/1910.09387">arXiv</a>
 
   <a href="https://ieeexplore.ieee.org/document/9052990">ieeexplore</a>
   </dd>
 
   <dt>BibTex entry</dt>

   <dd>
 
	@inproceedings{drossos:2020:icassp,
	title={Clotho: An Audio Captioning Dataset},
	author={Drossos, K. and Lipping, S. and Virtanen, T.},
	booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	pages={736--740},
	year={2020}}
 
   </dd>
 </dl>

----

</details>

---

# Year 2019

#### Crowdsourcing a Dataset of Audio Captions

<details><summary>Information</summary>
 <dl>
   <dt>Reference</dt>
   <dd>S. Lipping, K. Drossos, and T. Virtanen, "Crowdsourcing a
   dataset of audio captions," in Detection and Classification of
   Acoustic Scenes and Events (DCASE) 2019, Oct. 2019
   </dd>
 
   <dt>Paper links</dt>
   <dd>
   <a href="https://arxiv.org/abs/1907.09238">arXiv</a>
 
   <a href="http://dcase.community/documents/workshop2019/proceedings/DCASE2019Workshop_Lipping_31.pdf">DCASE</a>
   </dd>
 
   <dt>BibTex entry</dt>

   <dd>
 
    @inproceedings{lipping:2019:dcase,
    author={S. Lipping and K. Drossos and T. Virtanen},
    title={Crowdsourcing a Dataset of Audio Captions},
	booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019)},
    address = {New York University, NY, USA},
    month = {Oct.},
    year = {2019},
    pages = {139--143},
    ISSN={2379-190X}}
 
   </dd>
 </dl>

----

</details>

#### Neural Audio Captioning Based On Conditional Sequence-to-Sequence Model

<details><summary>Information</summary><br>

 <dl>
   <dt>Reference</dt>
   <dd>Shota Ikawa and Kunio Kashino, "Neural Audio Captioning Based
   On Conditional Sequence-to-Sequence Model," in Workshop of Detection
   and Classification of Acoustic Scenes and Events (DCASE), Oct.
   2019.
   </dd>
 
   <dt>Paper links</dt>
   <dd>
   <a href="http://dcase.community/documents/workshop2019/proceedings/DCASE2019Workshop_Ikawa_82.pdf">DCASE</a>
   </dd>
 
   <dt>BibTex entry</dt>

   <dd>
 
    @inproceedings{ikawa:2019:dcase,
    author = {Shota Ikawa and Kunio Kashino},
    title = {Neural Audio Captioning Based On Conditional Sequence-to-Sequence Model},
    booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events 2019 Workshop ({DCASE2019})},
    address = {New York University, NY, USA},
    month = {Oct.},
    year = {2019},
    pages = {99--103},
	ISSN={2379-190X}}
 
   </dd>
 </dl>

----

</details>
 
#### AudioCaps: Generating captions for audios in the wild

<details><summary>Information</summary><br>

 <dl>
   <dt>Reference</dt>
   <dd>C. D. Kim, B. Kim, H. Lee, and G. Kim, "AudioCaps:
   Generating captions for audios in the wild,” in Proceedings
   of the 2019 Conference of the North American Chapter of the
   Association for Computational Linguistics: Human Language
   Technologies, Volume 1 (Long and Short Papers), Minneapolis,
   Minnesota, Jun. 2019, pp. 119–132, Association for Computational
   Linguistics 
   </dd>
 
   <dt>Paper links</dt>
   <dd>
   <a href="https://arxiv.org/abs/1706.10006">acwlweb</a>
   </dd>
 
   <dt>Code</dt>
   <dd>
   <a href="https://github.com/cdjkim/audiocaps">GitHub</a>
   </dd>
 
   <dt>Data</dt>
   <dd>
   <a href="https://github.com/cdjkim/audiocaps/blob/master/dataset/README.md">GitHub</a>
   </dd>
 
   <dt>BibTex entry</dt>

   <dd>
 
    @inproceedings{kim:2019:nacacl,
    title = {{A}udio{C}aps: Generating Captions for Audios in The Wild},
    author = {C. D. Kim and B. Kim and H. Lee and G. Ki}",
    booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
    month = {Jun.},
    year = {2019},
    address = {Minneapolis, Minnesota},
    publisher = {Association for Computational Linguistics},
    doi = {10.18653/v1/N19-1011},
    pages = {119--132}}
 
   </dd>
 </dl>

----

</details>

#### Audio caption: Listen and tell

<details><summary>Information</summary><br>

 <dl>
   <dt>Reference</dt>
   <dd>M. Wu, H. Dinkel, and K. Yu, "Audio caption: Listen and
   tell," in 2019 IEEE International Conference on Acoustics,
   Speech and Signal Processing (ICASSP), May 2019, pp. 830–834
   </dd>
 
   <dt>Paper links</dt>
   <dd>
   <a href="https://arxiv.org/abs/1706.10006">arXiv</a>
 
   <a href="https://ieeexplore.ieee.org/document/8170058">ieeexplore</a>
   </dd>
 
   <dt>BibTex entry</dt>

   <dd>

    @inproceedings{wu:2019:icassp,
    author={M. {Wu} and H. {Dinkel} and K. {Yu}},
    booktitle={2019 IEEE International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
    title={Audio Caption: Listen and Tell},
    year={2019},
    pages={830-834},
    doi={10.1109/ICASSP.2019.8682377},
    ISSN={2379-190X},
    month={May}}

   </dd>
 </dl>

----

</details>

----

# Year 2017

#### Automated Audio Captioning with Recurrent Neural Networks


<details><summary>Information</summary><br>

 <dl>
   <dt>Reference</dt>
   <dd>K. Drossos, S. Adavanne, and T. Virtanen, "Automated audio
   captioning with recurrent neural networks," in 2017 IEEE Workshop
   on Applications of Signal Processing to Audio and Acoustics
   (WASPAA), Oct. 2017, pp. 374–378</dd>
 
   <dt>Paper links</dt>
   <dd>
   <a href="https://arxiv.org/abs/1706.10006">arXiv</a>
 
   <a href="https://ieeexplore.ieee.org/document/8170058">ieeexplore</a>
   </dd>
 
   <dt>BibTex entry</dt>
 
   <dd>

    @inproceedings{drossos:2017:waspaa,
    author={K. {Drossos} and S. {Adavanne} and T. {Virtanen}},
    booktitle={2017 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},
    title={Automated audio captioning with recurrent neural networks},
    year={2017},
    pages={374-378}}
 
   </dd>
 </dl>

----

</details>

